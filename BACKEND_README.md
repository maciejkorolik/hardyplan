# Backend Implementation - Complete

## ‚úÖ What's Been Implemented

### 1. **Data Models & Validation** (`lib/types.ts`)

- `TrainingSession`: Single training session structure
- `DaySchedule`: Daily training schedule
- `WeekSchedule`: Full week schedule with metadata
- `ScrapingResult`: Tracking for scraping operations
- All models use Zod for runtime validation

### 2. **Upstash Redis Storage** (`lib/kv.ts`)

Storage utilities using Upstash Redis (Vercel's recommended provider):

- `storeWeekSchedule()`: Store weekly schedules with timestamp
- `weekScheduleExists()`: Check for duplicates
- `getWeekSchedule()`: Retrieve specific week
- `getAllWeekSchedules()`: Get all schedules (sorted)
- `getLatestWeekSchedule()`: Get most recent week
- `getDaySchedule()`: Find schedule by date (DD.MM)
- `getTodaySchedule()`: Get today's schedule
- `logScrapingOperation()`: Log scraping results
- `shouldSkipScraping()`: **NEW** Smart scheduling to avoid unnecessary scrapes

**Storage Keys:**

- `schedules:{week}` ‚Üí Full week schedule (JSON)
- `schedules:list` ‚Üí Sorted set of all weeks
- `schedules:latest` ‚Üí Pointer to latest week
- `schedules:latest_update` ‚Üí **NEW** Timestamp of last update
- `logs:scraping:{date}` ‚Üí Daily logs (30 day TTL)

**Smart Scheduling:**

Automatically skips scraping if:

- You have 14+ days of schedules ahead
- Last update was less than 5 days ago
- Respects gym's posting schedule (current week + next week)

### 3. **Firecrawl Integration** (`lib/firecrawl.ts`)

Web scraping utilities:

- `scrapeCategoryPage()`: Extract blog post URLs from category page
- `scrapeBlogPost()`: Scrape markdown from blog post
- `scrapeAllBlogPosts()`: Orchestrate full scraping flow
- Filters for relevant blog posts (plan-treningowy)
- Returns top 2 most recent posts

### 4. **LLM Parsing** (`lib/llm-parser.ts`)

AI-powered schedule parsing:

- Uses Google Gemini 2.0 Flash (via Vercel AI SDK)
- Structured output with Zod schema validation
- Complete parsing prompt from PRD included
- `parseScheduleWithLLM()`: Parse single schedule
- `parseAllSchedules()`: Batch processing with error handling
- Automatic retry on failure (max 2 retries)

### 5. **API Endpoints**

#### **POST /api/scrape**

Daily cron job endpoint with smart scheduling:

- Requires `Bearer {CRON_SECRET}` authorization
- **Smart skip**: Checks if scraping is needed before starting
- Scrapes blog ‚Üí Parses with LLM ‚Üí Stores in Redis
- Duplicate detection (skips existing weeks)
- Comprehensive logging
- Returns: success status, schedules processed, errors, URLs, skip reason (if skipped)

#### **GET /api/schedules/today**

Returns today's schedule:

- Automatically calculates current date
- Returns: `DaySchedule` object or `null`
- Cache: 5 minutes

#### **GET /api/schedules**

Returns all schedules or specific week:

- Query param: `?week=DD/MM-DD/MM` (optional)
- Returns: Array of `WeekSchedule` objects or single schedule
- Sorted by date (newest first)
- Cache: 1 hour

#### **GET /api/schedules/[date]**

Returns schedule for specific date:

- Route param: date in DD.MM format
- Validates date format
- Returns: `DaySchedule` object or `null`
- Cache: 1 hour

## üì¶ Installed Dependencies

```json
{
  "@upstash/redis": "1.35.6",
  "ai": "5.0.76",
  "@ai-sdk/google": "2.0.23",
  "firecrawl": "4.4.1",
  "zod": "4.1.12"
}
```

**Note**: Uses Upstash Redis (Vercel's recommended KV storage provider)

## üîß Configuration Files

### **vercel.json**

```json
{
  "crons": [
    {
      "path": "/api/scrape",
      "schedule": "0 6 * * *"
    }
  ]
}
```

Runs daily at 6:00 AM UTC (8:00 AM Poland time)

### **Environment Variables**

See `ENV_SETUP.md` for complete setup guide.

Required variables:

- `FIRECRAWL_API_KEY`
- `GOOGLE_GENERATIVE_AI_API_KEY`
- `UPSTASH_REDIS_REST_URL`, `UPSTASH_REDIS_REST_TOKEN`
- `CRON_SECRET`
- `BLOG_CATEGORY_URL`

## üöÄ Testing the Backend

### 1. Set up environment variables

Create `.env` file with all required keys (see `ENV_SETUP.md`)

### 2. Set up Upstash Redis

```bash
# Option 1: Use existing Upstash account
# Go to https://console.upstash.com
# Create Redis database and copy credentials to .env

# Option 2: Use Vercel CLI (auto-creates Upstash)
pnpm add -g vercel
vercel link
vercel env pull .env
```

### 3. Test scraping endpoint

```bash
# Start dev server
pnpm dev

# Trigger scraping (in another terminal)
curl -X POST http://localhost:3000/api/scrape \
  -H "Authorization: Bearer YOUR_CRON_SECRET"
```

### 4. Test schedule endpoints

```bash
# Get today's schedule
curl http://localhost:3000/api/schedules/today

# Get all schedules
curl http://localhost:3000/api/schedules

# Get specific week
curl "http://localhost:3000/api/schedules?week=20/10-26/10"

# Get specific date
curl http://localhost:3000/api/schedules/20.10
```

## üìù Data Flow

```
Blog Category Page
    ‚Üì (Firecrawl)
Extract Blog Post URLs
    ‚Üì (Firecrawl)
Scrape Markdown Content
    ‚Üì (Vercel AI SDK + Gemini)
Parse to Structured JSON
    ‚Üì (Zod Validation)
Validated WeekSchedule
    ‚Üì (Duplicate Check)
Store in Vercel KV
    ‚Üì
API Endpoints ‚Üí Frontend
```

## üéØ Key Features

‚úÖ Automatic daily scraping with smart scheduling
‚úÖ AI-powered parsing with Gemini 2.0 Flash
‚úÖ Duplicate prevention
‚úÖ **NEW** Smart skip logic (saves API costs)
‚úÖ Upstash Redis storage (Vercel-recommended)
‚úÖ Comprehensive error handling
‚úÖ Request caching for performance
‚úÖ Polish language error messages
‚úÖ Operation logging
‚úÖ Type-safe throughout

## üöÄ Performance Optimizations

### Smart Scraping Algorithm

The system intelligently determines when to skip scraping:

1. **Calculates schedule coverage**: Checks how many days ahead we have schedules
2. **Checks update frequency**: Tracks last successful update
3. **Skip conditions**:
   - Has 14+ days of schedules
   - Last update was < 5 days ago
4. **Respects gym schedule**: They post current week + next week, then replace

**Example scenario:**

- Monday: Scrapes 2 weeks (14 days ahead)
- Tuesday-Friday: Skips (already have 13-10 days ahead, last update < 5 days)
- Next Monday: Scrapes again (current week ended, need new schedule)

**Benefits:**

- Reduces Firecrawl API calls by ~70%
- Reduces LLM API costs by ~70%
- Still catches updates within 5 days max

## üîú Next Steps

The backend is complete and ready. Next phase:

1. Build frontend components
2. Create homepage with today's schedule
3. Add navigation and history views
4. Deploy to Vercel
5. Test cron execution in production

## üìö File Structure

```
lib/
  ‚îú‚îÄ‚îÄ types.ts          # Data models & Zod schemas
  ‚îú‚îÄ‚îÄ kv.ts             # Vercel KV storage utilities
  ‚îú‚îÄ‚îÄ firecrawl.ts      # Web scraping logic
  ‚îî‚îÄ‚îÄ llm-parser.ts     # AI parsing with Vercel AI SDK

app/api/
  ‚îú‚îÄ‚îÄ scrape/
  ‚îÇ   ‚îî‚îÄ‚îÄ route.ts      # POST /api/scrape (cron endpoint)
  ‚îî‚îÄ‚îÄ schedules/
      ‚îú‚îÄ‚îÄ route.ts      # GET /api/schedules
      ‚îú‚îÄ‚îÄ today/
      ‚îÇ   ‚îî‚îÄ‚îÄ route.ts  # GET /api/schedules/today
      ‚îî‚îÄ‚îÄ [date]/
          ‚îî‚îÄ‚îÄ route.ts  # GET /api/schedules/[date]
```

---

**Status**: ‚úÖ Backend Implementation Complete
**Next Phase**: Frontend Development
**Estimated Time**: Ready for frontend work
